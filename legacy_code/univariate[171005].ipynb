{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import scipy.integrate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from itertools import chain\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When code is in script, we define the path of the script's parent folder location as the 'root' directory\n",
    "# From this 'root' we can travel to the relevant folders with minimal adjustment\n",
    "try:\n",
    "    root = os.path.dirname(os.path.realpath(__file__))\n",
    "    root = \"/\".join(root.split('\\\\')[:-1])\n",
    "    log.append('Dynamic paths'+'\\n')\n",
    "except:\n",
    "    root = 'C:/wamp64/www/python_predictions_4/assets/scripts'\n",
    "    log.append('Static paths'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To allow pandas dataframes to display more columns\n",
    "pd.set_option(\"display.max_columns\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and organize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basetable and its types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Types csv file CAN be defined to be used to convert variables (of the basetable, see below) to the desired data types\n",
    "# The Types csv files should include one column with variable names and one column with desired types (e.g. int,float,str,bool)\n",
    "# If no Types csv file is provided no convertions will be forced. In that case 'Python' will guess the data type of each column \n",
    "types_path = root+\"/python/data_types.csv\"\n",
    "types_exist = True\n",
    "\n",
    "try:    \n",
    "    df_types = pd.read_csv(types_path, header=None)\n",
    "    bool_mask = df_types[1]!='bool'\n",
    "    # Extract the functions based on the given type (e.g. 'str' -> str, 'int' -> int), for proper convertion \n",
    "    df_types.loc[bool_mask,1] = [getattr(__builtins__, type_str) for type_str in df_types.loc[bool_mask,1]]\n",
    "    # A type 'bool' is also attributed the function str, for convertion\n",
    "    df_types.loc[bool_mask==False,1] = getattr(__builtins__, 'str')\n",
    "    #types = df_types[bool_mask].set_index(0).T.to_dict('records')\n",
    "    types = df_types.set_index(0).T.to_dict('records')\n",
    "except FileNotFoundError:\n",
    "    types = [dict()]\n",
    "    types_exist = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basetable csv file should have column names as its first row\n",
    "# The columns names should include 'TARGET', 'ID'\n",
    "data_path = root+\"/python/data.csv\"\n",
    "\n",
    "df_in = pd.read_csv(data_path\n",
    "                    ,header=0\n",
    "                    ,sep=None\n",
    "                    ,engine='python'\n",
    "                    ,converters=types[0])\n",
    "\n",
    "# If no Types csv file was provided pd.read_csv guessed the types, we now output these types in a csv for re-use & later use\n",
    "if types_exist == False:\n",
    "    filename = root+\"/python/data_types.csv\"\n",
    "    funtotype = lambda x:re.findall('[a-z]+',str(x))[0].replace('object','str')\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        write=csv.writer(csvfile, delimiter =',')\n",
    "        write.writerows([column\n",
    "                         ,funtotype(df_in[column].dtype)] for column in df_in.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove quotes from variable names and/or variable values\n",
    "def strip_quot(x_in):\n",
    "    try:\n",
    "        x_out = x_in.strip().strip('\"').strip(\"'\")\n",
    "    except:\n",
    "        x_out=x_in\n",
    "    return x_out\n",
    "\n",
    "# Function to put 'id' and 'target' variable names in uppercase, all other variable names are put in lowercase\n",
    "# This is coded as to visually differentiate predictors from other variables\n",
    "# But another combination of upper/lower is possible as well, e.g. all variable names in uppercase\n",
    "def lower_upper(x_in):\n",
    "    if ((x_in.lower() == 'id')|(x_in.lower() == 'target')):\n",
    "        x_out = x_in.upper()\n",
    "    else:\n",
    "        x_out = x_in.lower()\n",
    "    return x_out\n",
    "\n",
    "# Function to group variable names based on the data type of the variable\n",
    "# Could as well use the types in Types.csv\n",
    "def get_headers(dataframe,type): \n",
    "    return dataframe.select_dtypes(include=[type]).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up quotes from column names\n",
    "df_in = df_in.rename(columns=strip_quot)\n",
    "\n",
    "# Perform uppercase/lowercase transformation to column names\n",
    "df_in = df_in.rename(columns=lower_upper)\n",
    "\n",
    "# Clean up quotes from column values\n",
    "df_in = df_in.applymap(strip_quot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group variable (names) based on the respective data type of each variable\n",
    "# With this information we know which variables are destined for equifrequency, regrouping or simply passing (see further)\n",
    "other_headers = [n for n in [\"TARGET\",\"ID\"]]\n",
    "try:\n",
    "    bool_headers = [n for n in df_types.loc[bool_mask==False,0].values if n not in other_headers]\n",
    "except:\n",
    "    bool_headers = []\n",
    "object_headers = [n for n in get_headers(df_in,'object') if n not in other_headers+bool_headers]\n",
    "numeric_headers = [n for n in get_headers(df_in,'number') if n not in other_headers+bool_headers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import settings defined by the user\n",
    "df_settings = pd.read_csv(root+'/python/analysis_settings.csv', sep=',', index_col=0, header=None).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make partition sets (based on desired user settings) for both targets (0 & 1) \n",
    "def partitionList(train_setting,selection_setting,validation_setting,sorted_target):\n",
    "    settings = {'train':train_setting,'selection':selection_setting,'validation':validation_setting}\n",
    "    parts = ['train','selection','validation']\n",
    "    result = []\n",
    "    for target in [sorted_target.iloc[0],sorted_target.iloc[-1]]:\n",
    "        target_length = (sorted_target==target).sum()\n",
    "        for part in parts:\n",
    "            result.extend( [part]*math.ceil(target_length*settings[part][1]/100) )    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and sort on TARGET\n",
    "df_in = df_in.iloc[np.random.permutation(len(df_in))].sort_values(by='TARGET', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Create partition based on analysis_setting.csv\n",
    "partition = partitionList(train_setting=df_settings.loc[:,'partitioning_train']\n",
    "                          ,selection_setting=df_settings.loc[:,'partitioning_selec']\n",
    "                          ,validation_setting=df_settings.loc[:,'partitioning_valid']\n",
    "                          ,sorted_target=df_in.TARGET) \n",
    "\n",
    "# Attach to dataframe\n",
    "df_in[\"PARTITION\"] = partition[:len(df_in)]\n",
    "\n",
    "# Sampling based on analysis settings (if both sampling_settings are set to 100, all data is used)\n",
    "sampling_settings = {1:df_settings.sampling_1, 0:df_settings.sampling_0}\n",
    "if (int(sampling_settings[1])<100) | (int(sampling_settings[0])<100):\n",
    "    for sample in sampling_settings:\n",
    "        sample_length = int(round((df_in.TARGET==sample).sum() * sampling_settings[sample]/100))\n",
    "        drop_index = df_in[df_in.TARGET==sample].index[sample_length:]\n",
    "        df_in.drop(drop_index,inplace=True)\n",
    "    df_in.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create output dataframe which will contain transformed variables\n",
    "df_out = df_in.loc[:,[\"ID\",\"TARGET\",\"PARTITION\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization function for Continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This function is a reworked version of pd.qcut to satisfy our particular needs\n",
    "### Takes for var a continuous pd.Series as input and returns a pd.Series with bin-labels (e.g. [4,6[ )\n",
    "### Train takes a series/list of booleans (note: we define bins based on the training set)\n",
    "### Autobins reduces the number of bins (starting from nbins) as a function of the number of missings\n",
    "### Nbins is the wished number of bins\n",
    "### Precision=0 results in integer bin-labels if possible\n",
    "### twobins=True forces the function to output at least two bins\n",
    "### catchLarge tests if some groups (or missing group) are very large, and if so catches and outputs two groups\n",
    "#### note: catchLarge makes twobins irrelevant\n",
    "\n",
    "def eqfreq(var, train, autobins=True, nbins=10, precision=0, twobins=True, catchLarge=True):\n",
    "    \n",
    "    \n",
    "    # Test for large groups and if one exists pass them with two bins: Large_group,Other\n",
    "    if catchLarge:\n",
    "        catchPercentage=1-(1/nbins)\n",
    "        groupCount = var[train].groupby(by=var[train]).count()\n",
    "        maxGroupPerc = groupCount.max()/len(var[train])\n",
    "        missingPerc = sum(var[train].isnull())/len(var[train])\n",
    "        if maxGroupPerc>=catchPercentage:\n",
    "            largeGroup = groupCount.sort_values(ascending=False).index[0]\n",
    "            x_binned = var.copy()\n",
    "            x_binned.name = 'B_'+var.name\n",
    "            x_binned[x_binned!=largeGroup]='Other'\n",
    "            cutpoints=None\n",
    "            info = (var.name+\": One large group, outputting 2 groups\")\n",
    "            return x_binned, cutpoints, info\n",
    "        elif missingPerc>=catchPercentage:\n",
    "            x_binned = var.copy()\n",
    "            x_binned.name = 'B_'+var.name\n",
    "            x_binned[x_binned.isnull()]='Missing'\n",
    "            x_binned[x_binned!='Missing']='Other'\n",
    "            cutpoints=None\n",
    "            info = (var.name+\": One large missing group, outputting 2 groups\")\n",
    "            return x_binned, cutpoints, info\n",
    "    # Adapt number of bins as a function of number of missings\n",
    "    if autobins:\n",
    "        length = len(var[train])\n",
    "        missing_total = var[train].isnull().sum()\n",
    "        missing_perten = missing_total/length*10\n",
    "        nbins = max(round(10-missing_perten)*nbins/10 ,1)\n",
    "    # Store the name and index of the variable\n",
    "    name = var.name\n",
    "    series_index = var.index\n",
    "    # Transform var and train to a np.array and list respectively, which is needed for some particular function&methods\n",
    "    x = np.asarray(var)\n",
    "    train = list(train)\n",
    "    # First step in finding the bins is determining what the quantiles are (named as cutpoints)\n",
    "    # If the quantile lies between 2 points we use lin interpolation to determine it\n",
    "    cutpoints = var[train].quantile(np.linspace(0,1,nbins+1),interpolation = 'linear')\n",
    "    # If the variable results only in 2 unique quantiles (due to skewness) increase number of quantiles until more than 2 bins can be formed\n",
    "    if twobins:\n",
    "        extrasteps = 1\n",
    "        # Include a max. extrasteps to avoid infinite loop\n",
    "        while (len(cutpoints.unique())<=2) & (extrasteps<20):\n",
    "            cutpoints = var[train].quantile(np.linspace(0,1,nbins+1+extrasteps),interpolation = 'linear')\n",
    "            extrasteps+=1\n",
    "    # We store which rows of the variable x lies under/above the lowest/highest cutpoint \n",
    "    # Without np.errstate(): x<cutpoints.min() or x>cutpoints.max() can give <RuntimeWarning> if x contains nan values (missings)\n",
    "    # However the function will result in False in both >&< cases, which is a correct result, so the warning can be ignored\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        under_lowestbin = x < cutpoints.min()\n",
    "        above_highestbin= x > cutpoints.max()\n",
    "\n",
    "\n",
    "    def _binnedx_from_cutpoints(x, cutpoints, precision, under_lowestbin, above_highestbin):\n",
    "    ### Attributes the correct bin ........................\n",
    "    ### Function that, based on the cutpoints, seeks the lowest precision necessary to have meaningful bins\n",
    "    ###  e.g. (5.5,5.5] ==> (5.51,5.54]\n",
    "    ### Attributes those bins to each value of x, to achieve a binned version of x   \n",
    "        \n",
    "        # Store unique cutpoints (e.g. from 1,3,3,5 to 1,3,5) to avoid inconsistensies when bin-label making\n",
    "        # Indeed, bins [...,1], (1,3], (3,3], (3,5], (5,...] do not make much sense\n",
    "        # While, bins  [...,1], (1,3],        (3,5], (5,...] do make sense\n",
    "        unique_cutpoints = cutpoints.unique()\n",
    "        # If there are only 2 unique cutpoints (and thus only one bin will be returned), \n",
    "        # keep original values and code missings as 'Missing'\n",
    "        if len(unique_cutpoints) <= 2:\n",
    "            cutpoints = None\n",
    "            x_binned = pd.Series(x)\n",
    "            x_binned[x_binned.isnull()] = 'Missing'\n",
    "            info = (var.name+\": Only one resulting bin, keeping original values instead\")\n",
    "            return x_binned, cutpoints, info\n",
    "        # Store info on whether or not the number of resulting bins equals the desired number of bins\n",
    "        elif len(unique_cutpoints) < len(cutpoints):\n",
    "            info = (var.name+\": Resulting # bins < whished # bins\")\n",
    "        else:\n",
    "            info = (var.name+\": Resulting # bins as desired\")\n",
    "        # Finally, recode the cutpoints (which can have doubles) as the unique cutpoints\n",
    "        cutpoints = unique_cutpoints\n",
    "        \n",
    "        # Store missing values in the variable as a mask, and create a flag to test if there are any missing in the variable\n",
    "        na_mask = np.isnan(x)\n",
    "        has_nas = na_mask.any()\n",
    "        # Attribute to every x-value the index of the cutpoint (from the sorted cutpoint list) which is equal or higher than\n",
    "        # the x-value, effectively encompasing that x-value.\n",
    "        # e.g. for x=6 and for sorted_cutpoint_list=[0,3,5,8,...] the resulting_index=3    \n",
    "        ids = cutpoints.searchsorted(x, side='left')\n",
    "        # x-values equal to the lowest cutpoint will recieve a ids value of 0\n",
    "        # but our code to attribute bins to x-values based on ids (see end of this subfunction) requires a min. value of 1\n",
    "        ids[x == cutpoints[0]] = 1\n",
    "        # Idem as previous: x-values below the lowest cutpoint should recieve a min. value of 1\n",
    "        if under_lowestbin.any():\n",
    "            ids[under_lowestbin] = 1\n",
    "        # Similar as previous: x-values above the highest cutpoint should recieve the max. allowed ids\n",
    "        if above_highestbin.any():\n",
    "            max_ids_allowed = ids[(above_highestbin == False) & (na_mask==False)].max()\n",
    "            ids[above_highestbin] = max_ids_allowed\n",
    "        # Maximal ids can now be defined if we neglect ids of missing values\n",
    "        max_ids = ids[na_mask==False].max()\n",
    "        \n",
    "        # Based on the cutpoints create bin-labels\n",
    "        # Iteratively go through each precision (= number of decimals) until meaningful bins are formed\n",
    "        # If theoretical bin is ]5.51689,5.83654] we will prefer ]5.5,5.8] as output bin\n",
    "        increases = 0\n",
    "        original_precision = precision\n",
    "        while True:\n",
    "            try:\n",
    "                bins = _format_bins(cutpoints, precision)\n",
    "            except ValueError:\n",
    "                increases += 1\n",
    "                precision += 1\n",
    "                #if increases >= 5:\n",
    "                    #warnings.warn(\"Modifying precision from \"+str(original_precision)+\" to \"+str(precision)+\" to achieve discretization\")\n",
    "                    #print(\"Modifying precision from \"+str(original_precision)+\" to \"+str(precision)+\" to achieve discretization\")\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Make array of bins to allow vector-like attribution\n",
    "        bins = np.asarray(bins, dtype=object)\n",
    "        # If x has nas: for each na-value, set the ids-value to max_ids+1\n",
    "        # this will allow na-values to be attributed the highest bin which we define right below\n",
    "        if has_nas:\n",
    "            np.putmask(ids, na_mask, max_ids+1)\n",
    "        # The highest bin is defined as 'Missing'\n",
    "        bins = np.append(bins,'Missing')\n",
    "        # ids-1 is used as index in the bin-labels list to attribute a bin-label to each x. Example:\n",
    "        # x=6   sorted_cutpoint_list=[0,3,5,8,...]   ids=3   levels=[[0,3],(3,5],(5,8],...]\n",
    "        # The correct bin level for x is (5,8] which has index 2 which is equal to the ids-1\n",
    "        x_binned = bins[ids-1]\n",
    "        return x_binned, cutpoints, info\n",
    "        \n",
    "\n",
    "    def _format_bins(cutpoints, prec):\n",
    "    # Based on the quantile list create bins. Raise error if values are similar within one bin.\n",
    "    # On error _binnedx_from_cutpoints will increase precision\n",
    "        \n",
    "        fmt = lambda v: _format_label(v, precision=prec)\n",
    "        bins = []\n",
    "        for a, b in zip(cutpoints, cutpoints[1:]):\n",
    "            fa, fb = fmt(a), fmt(b)\n",
    "            \n",
    "            if a != b and fa == fb:\n",
    "                raise ValueError('precision too low')\n",
    "                \n",
    "            formatted = '(%s, %s]' % (fa, fb)\n",
    "            bins.append(formatted)\n",
    "        \n",
    "        bins[0] = '[...,' + bins[0].split(\",\")[-1]\n",
    "        bins[-1] = bins[-1].split(\",\")[0] + ',...]'\n",
    "        return bins\n",
    "\n",
    "\n",
    "    def _format_label(x, precision):\n",
    "    # For a specific precision, returns the value formatted with the appropriate amount of numbers after comma and correct brackets\n",
    "    \n",
    "        if isinstance(x,float):\n",
    "            frac, whole = np.modf(x)\n",
    "            sgn = '-' if x < 0 else ''\n",
    "            whole = abs(whole)\n",
    "            if frac != 0.0:\n",
    "                val = '{0:.{1}f}'.format(frac, precision)\n",
    "                val = _trim_zeros(val)\n",
    "                if '.' in val:\n",
    "                    return sgn + '.'.join(('%d' % whole, val.split('.')[1]))\n",
    "                else: \n",
    "                    if '0' in val:\n",
    "                        return sgn + '%0.f' % whole\n",
    "                    else:\n",
    "                        return sgn + '%0.f' % (whole+1)\n",
    "            else:\n",
    "                return sgn + '%0.f' % whole\n",
    "        else:\n",
    "            return str(x)\n",
    "\n",
    "\n",
    "    def _trim_zeros(x):\n",
    "    # Removes unnecessary zeros and commas\n",
    "        while len(x) > 1 and x[-1] == '0':\n",
    "            x = x[:-1]\n",
    "        if len(x) > 1 and x[-1] == '.':\n",
    "            x = x[:-1]\n",
    "        return x\n",
    "\n",
    "\n",
    "    x_binned, cutpoints, info = _binnedx_from_cutpoints(x, cutpoints, precision=precision, under_lowestbin=under_lowestbin, above_highestbin=above_highestbin)\n",
    "    x_binned = pd.Series(x_binned, index=series_index, name=\"B_\"+name)\n",
    "    return x_binned, cutpoints, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for n in numeric_headers:\n",
    "    result = eqfreq(var=df_in[n]\n",
    "                    ,train=df_in[\"PARTITION\"]==\"train\"\n",
    "                    ,autobins=True\n",
    "                    ,nbins=int(df_settings.discretization_nbins)\n",
    "                    ,precision=0\n",
    "                    ,twobins=True\n",
    "                    ,catchLarge=True)\n",
    "    print(n)\n",
    "    print(result[0].unique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function to continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "# We loop only through the numeric variables\n",
    "for n in numeric_headers:\n",
    "    # Perform the equifrequency function\n",
    "    result = eqfreq(var=df_in[n]\n",
    "                    ,train=df_in[\"PARTITION\"]==\"train\"\n",
    "                    ,autobins=True\n",
    "                    ,nbins=int(df_settings.discretization_nbins)\n",
    "                    ,precision=0\n",
    "                    ,twobins=True\n",
    "                    ,catchLarge=False) # TRUE OPTION STILL PRODUCES ERROR IN SORTNUMERIC function AND SCORING procedure !!!!!!!!!\n",
    "    df_out = pd.concat([df_out,result[0]], axis=1)\n",
    "    log.append(result[2])\n",
    "toc = time.time()\n",
    "log.append(\"Discretisation: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for labeling missing/empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check which values of a var are empty strings or null values\n",
    "def maskmissing(var):\n",
    "    # Check if values are null\n",
    "    crit1 = var.isnull()\n",
    "    # Check if values are empty strings\n",
    "    modvar = pd.Series([str(value).strip() for value in var])\n",
    "    crit2 = modvar==pd.Series(['']*len(var))\n",
    "    #crit2 = var==pd.Series(['']*len(var))\n",
    "    #crit3 = var==pd.Series([' ']*len(var))\n",
    "    return crit1 | crit2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regrouping Function for nominal/ordinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouping function for categorical variables\n",
    "# Each group is tested with a chi² for relevant incidence differences in comparison to a rest-group\n",
    "# The rest group has the size of the remaining groups and an 'overall average incidence' (if dummy=True) or \n",
    "# 'remaining groups average incidence' (if dummy=False)\n",
    "# Groups with a pvalue above the threshold are relabled to a single group\n",
    "\n",
    "def regroup(var,target,train,pval_thresh=0.01,dummy=True,keep='Missing',rename='Other'):\n",
    "    \n",
    "    # Define the chi² test condition\n",
    "    # Groups that do not meet the condition are not analyzed and will be unconditionally relabled\n",
    "    def _chi2cond_(var=var,target=target,train=train):\n",
    "        varcounts = var[train].groupby(by=var).count()\n",
    "        train_inc = target[train].sum()/len(target[train])\n",
    "        factor = max(train_inc, 1-train_inc)\n",
    "        analyze_mask = (varcounts*factor)>5\n",
    "        analyze_groups = analyze_mask.index[analyze_mask].values\n",
    "        return analyze_groups\n",
    "    \n",
    "    # Compute overal incidence mean\n",
    "    incidence_mean = target[train].mean()\n",
    "    # Create container of which groups will be kept, compared to the groups which will be relabled\n",
    "    keepgroups = []\n",
    "    # Cycle and test each group that meets the chi² condition\n",
    "    for group in _chi2cond_():\n",
    "        # Container for target 0/1 observations of the group under scrutiny\n",
    "        obs_group = []\n",
    "        # Counts of the target 0/1 occurences for the group under scrutiny\n",
    "        obs_group.append(((target[train]==0)&(var[train]==group)).sum())\n",
    "        obs_group.append(((target[train]==1)&(var[train]==group)).sum())\n",
    "        obs_group = np.array(obs_group)\n",
    "        # Container for target 0/1 observations of the remaining groups together\n",
    "        obs_other = []\n",
    "        # Counts of the target 0/1 occurences for the remaining groups together\n",
    "        obs_other.append(((target[train]==0)&(var[train]!=group)).sum())\n",
    "        obs_other.append(((target[train]==1)&(var[train]!=group)).sum())\n",
    "        obs_other = np.array(obs_other)\n",
    "        # If dummy=True, we scale the two groups of target 0/1 occurences such that the incidence is equal to the overall incidence\n",
    "        # The size of the two groups of target 0/1 occurences is still equal to the size of the remaining groups\n",
    "        if dummy:\n",
    "            obs_other_size = obs_other.sum()\n",
    "            obs_other[0]=(1-incidence_mean)*obs_other_size # 0(1) index coincides with target = 0(1)\n",
    "            obs_other[1]=(  incidence_mean)*obs_other_size\n",
    "        obs = np.array([obs_group,obs_other])\n",
    "        # Place at least 1 observation to avoid error in chi2 test\n",
    "        obs[obs==0] = 1\n",
    "        # Perform chi² test\n",
    "        pval = stats.chi2_contingency(obs, correction=False)[1]\n",
    "        # If pval outperforms threshold, append the group in the keepgroups list\n",
    "        if pval<=pval_thresh:\n",
    "            keepgroups.append(group)\n",
    "        #elif group==keep:\n",
    "        #    keepgroups.append(group)\n",
    "    # If the specific group to be kept (e.g. 'Missing') didn't pass the test, append it to the keepgroups list\n",
    "    if keep not in keepgroups:\n",
    "        keepgroups.append(keep)\n",
    "    # Makes a list of all groups not in the keepgroups list\n",
    "    regroup_mask = [val not in keepgroups for val in var.values]\n",
    "    var_regroup = var.copy()\n",
    "    # Rename those groups\n",
    "    var_regroup[regroup_mask] = rename\n",
    "    var_regroup.name = \"B_\"+var.name\n",
    "    info = (var.name+\": from \"+str(len(var.unique()))+\" to \"+str(len(var_regroup.unique())))\n",
    "    return var_regroup, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Apply function to nominal/ordinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "# We loop only through the categorical variables\n",
    "for h in object_headers:\n",
    "    # We label missing and empty values for categorical variables as 'Missing'\n",
    "    # Note the interaction with the 'keep' parameter of the regroup function.\n",
    "    mask = maskmissing(df_in[h])\n",
    "    df_in.loc[mask,h]='Missing'\n",
    "    # Perform regrouping function\n",
    "    result = regroup(var=df_in[h]\n",
    "                     ,target=df_in.loc[:,'TARGET']\n",
    "                     ,train=df_in.PARTITION=='train'\n",
    "                     ,pval_thresh=float(df_settings.regrouping_signif)\n",
    "                     ,dummy=True\n",
    "                     ,keep='Missing'\n",
    "                     ,rename='Non-significants')\n",
    "    df_out = pd.concat([df_out,result[0]],axis=1)\n",
    "    log.append(result[1])\n",
    "toc = time.time()\n",
    "log.append(\"Regrouping: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of boolean variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Function to pass variables as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We could just rename them or put them with the regoup function, but for now let's keep consistent with the other functions\n",
    "def passvar(var):\n",
    "    var_pass = var.copy()\n",
    "    var_pass.name = \"B_\"+var.name\n",
    "    info = (\"Passing \"+var.name)\n",
    "    return var_pass, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "# We loop only through the boolean variables\n",
    "for b in bool_headers:\n",
    "    # We label missing and empty values for boolean variables as 'Missing'\n",
    "    mask = maskmissing(df_in[b])\n",
    "    df_in.loc[mask,b]='Missing'\n",
    "    # Perform the passvar function\n",
    "    result = passvar(var=df_in[b])\n",
    "    df_out = pd.concat([df_out,result[0]],axis=1)\n",
    "    log.append(result[1])\n",
    "toc = time.time()\n",
    "log.append(\"Passing: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Incidence Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Function for incidence replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def increp(b_var, target, train):    \n",
    "    #get variable name\n",
    "    name = b_var.name\n",
    "    #get overall incidence \n",
    "    incidence_mean = target[train].mean()\n",
    "    #get incidence per group\n",
    "    incidences = target[train].groupby(b_var).mean()\n",
    "    #construct dataframe with incidences\n",
    "    idf = pd.DataFrame(incidences).reset_index()\n",
    "    #get values that are in the data but not in the labels\n",
    "    bin_labels = incidences.index\n",
    "    newgroups = list(set(b_var.unique()) ^ set(bin_labels))\n",
    "    #if newgroups, add mean incidence to incidence dataframe for each new group\n",
    "    if len(newgroups)>0:\n",
    "        #make dataframe:\n",
    "        ngdf = pd.DataFrame(newgroups)\n",
    "        ngdf.columns = [name]\n",
    "        ngdf[\"TARGET\"] = incidence_mean\n",
    "        #dataframe with incidences:    \n",
    "        idf = idf.append(ngdf)\n",
    "    #dataframe with the variable\n",
    "    vdf = pd.DataFrame(b_var)\n",
    "    #discretized variable by merge\n",
    "    d_var = pd.merge(vdf,idf,how='left',on=name)[\"TARGET\"]\n",
    "    return pd.Series(d_var, name=\"D_\"+name[2:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function for incidence replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the columns destined for incidence replacement\n",
    "headers_for_incidrep = [h for h in df_out.columns if ((h not in ['ID','TARGET','PARTITION']) & (h[:2]==\"B_\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "# We loop only through the columns destined for incidence replacement\n",
    "for n in headers_for_incidrep:\n",
    "    # Perform increp function\n",
    "    result = increp(b_var=df_out[n]\n",
    "                    ,target=df_out.TARGET\n",
    "                    ,train=df_out.PARTITION==\"train\")\n",
    "    df_out = pd.concat([df_out,result], axis=1)\n",
    "    log.append(n+ \" processed\")\n",
    "toc = time.time()\n",
    "log.append(\"Incidence replacement: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AUCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for auc calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getauc(var, target, partition):     \n",
    "    \n",
    "    y = np.array(target[partition])\n",
    "    pred = np.array(var[partition])\n",
    "    pred = pred.astype(np.float64)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y,pred, pos_label=1)\n",
    "    \n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying function for auc calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define the columns for which an AUC score should be computed\n",
    "headers_for_auc = [h for h in df_out.columns if ((h not in ['ID','TARGET','PARTITION']) & (h[:2]==\"D_\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list_all = []\n",
    "parts = [\"train\",\"selection\"]\n",
    "tic = time.time()\n",
    "# We loop only through those columns for which an AUC score should be computed\n",
    "for header in headers_for_auc:\n",
    "    auc_list_var = [header[2:]]\n",
    "    # We loop through the two sets ('train' and 'selection') for which an AUC score is needed\n",
    "    for part in parts:\n",
    "        # Perform getauc function\n",
    "        auc_value = getauc(var=df_out[header]\n",
    "                           ,target=df_out.TARGET\n",
    "                           ,partition=df_out.PARTITION==part)\n",
    "        auc_list_var.append(auc_value.round(2)) #We round auc values to 2 decimals\n",
    "    auc_list_all.append(auc_list_var)\n",
    "    log.append(header + \" processed\")\n",
    "# We create a supplementary dataframe destined for Cobra input  \n",
    "df_auc = pd.DataFrame(auc_list_all,columns=['variable','AUC train','AUC test'])\n",
    "toc = time.time()\n",
    "log.append(\"Auc: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "# We identify those variables for which the AUC score is above the user-defined threshold\n",
    "auc_thresh = df_auc.loc[:,'AUC test'] > float(df_settings.preselection_auc)\n",
    "# We identify those variables for which the AUC score difference between 'train' and 'selection' is within the user-defined ratio\n",
    "auc_overtrain = (df_auc.loc[:,'AUC train']*100 - df_auc.loc[:,'AUC test']*100) < float(df_settings.preselection_overtrain)\n",
    "# Only those variables passing the 2 criteria above are preselected\n",
    "preselect = auc_thresh & auc_overtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a supplementary dataframe destined for Cobra input  \n",
    "df_variable_selections = pd.DataFrame({'variable':df_auc.variable\n",
    "                                      ,'preselect':preselect.astype(int)\n",
    "                                      ,'Default':np.zeros(len(preselect)).astype(int)\n",
    "                                      ,'Alternative 1':np.zeros(len(preselect)).astype(int)\n",
    "                                      ,'Alternative 2':np.zeros(len(preselect)).astype(int)\n",
    "                                      ,'Alternative 3':np.zeros(len(preselect)).astype(int)\n",
    "                                      ,'Alternative 4':np.zeros(len(preselect)).astype(int)\n",
    "                                      ,'Alternative 5':np.zeros(len(preselect)).astype(int)}\n",
    "                                     ,columns=['variable'\n",
    "                                               ,'preselect'\n",
    "                                               ,'Default'\n",
    "                                               ,'Alternative 1'\n",
    "                                               ,'Alternative 2'\n",
    "                                               ,'Alternative 3'\n",
    "                                               ,'Alternative 4'\n",
    "                                               ,'Alternative 5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,var in enumerate(df_variable_selections.variable):\n",
    "    log.append(var+\" \"+np.array(['passed','filtered'])[df_variable_selections.preselect][i])\n",
    "toc = time.time()\n",
    "log.append(\"Preselection: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the columns for which a correlation score should be computed\n",
    "headers_for_corr = [h for h in df_out.columns if ((h not in ['ID','TARGET','PARTITION']) & (h[:2]==\"D_\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_out.PARTITION==\"train\"\n",
    "tic = time.time()\n",
    "dataforcorr = np.transpose(np.matrix(df_out.loc[train,headers_for_corr],dtype=float))\n",
    "mat_corr = np.corrcoef(dataforcorr)\n",
    "toc = time.time()\n",
    "log.append(\"Correlations: \"+str(toc-tic)+\" sec\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame(mat_corr)\n",
    "df_corr.columns = headers_for_corr\n",
    "df_corr.index = headers_for_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of all Auc values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_path = root+'/data/univariate/aucs.csv'\n",
    "df_auc = df_auc.sort_values(by=['AUC test','AUC train'], ascending=False).reset_index(drop=True)\n",
    "df_auc.to_csv(path_or_buf=auc_path\n",
    "              ,sep=';'\n",
    "              ,index=False\n",
    "              ,encoding='utf-8'\n",
    "              ,line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables of Incidences & Correlations per variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for sorting cont.variables, whether or not they have undergone discritization\n",
    "def sortnumeric(dataframe):\n",
    "    \n",
    "    lowestnumber = 0\n",
    "    # If the variable was discretisized\n",
    "    if '[...' in [str(l)[:4] for l in dataframe.group.values]:\n",
    "        unsorted_labels = dataframe.group.values\n",
    "        label_items=[]\n",
    "        for label in unsorted_labels:\n",
    "            # For each bin label, retain the first value\n",
    "            label_items.append(label.split(\",\")[0].strip(\"[\").strip(\"(\"))\n",
    "        label_items=np.asarray(label_items)\n",
    "        # Special cases that are not numeric are given numbers\n",
    "        lowestnumber = label_items[(label_items!=\"...\")&(label_items!=\"Missing\")].astype('float64').min()\n",
    "        label_items[label_items=='...']= lowestnumber-1\n",
    "        label_items[label_items=='Missing']= lowestnumber-2\n",
    "        # argsort based on the numbers\n",
    "        rank = label_items.astype('float64').argsort()\n",
    "        return rank\n",
    "    \n",
    "    # If the variable wasn't discretisized, simply argsort on the numbers\n",
    "    else:\n",
    "        label_items = dataframe.group.values\n",
    "        if len(label_items)>1:\n",
    "            lowestnumber = label_items[label_items.astype('O')!=\"Missing\"].astype('float64').min()\n",
    "        label_items[label_items.astype('O')=='Missing']= lowestnumber-2\n",
    "        rank = label_items.astype('float64').argsort()\n",
    "        return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for sorting cont.variables, whether or not they have undergone discritization\n",
    "def sortnumeric_old(dataframe):\n",
    "    \n",
    "    # If the variable was discretisized\n",
    "    if dataframe.group.dtype=='object': #or# if np.array([str(unsorted_labels[i])[0] in [\"[\",\"(\",\"M\"] for i in range(0,len(unsorted_labels))]).all():\n",
    "        unsorted_labels = dataframe.group.values\n",
    "        label_items=[]\n",
    "        for label in unsorted_labels:\n",
    "            # For each bin label, retain the first value\n",
    "            label_items.append(label.split(\",\")[0].strip(\"[\").strip(\"(\"))\n",
    "        label_items=np.asarray(label_items)\n",
    "        # Special cases that are not numeric are given numbers\n",
    "        lowestnumber = label_items[(label_items!=\"...\")&(label_items!=\"Missing\")].astype('float64').min()\n",
    "        label_items[label_items=='...']= lowestnumber-1\n",
    "        label_items[label_items=='Missing']= lowestnumber-2\n",
    "        # argsort based on the numbers\n",
    "        rank = label_items.astype('float64').argsort()\n",
    "        return rank\n",
    "    \n",
    "    # If the variable wasn't discretisized, simply argsort on the numbers\n",
    "    else:\n",
    "        rank = dataframe.group.values.argsort()\n",
    "        return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for sorting cat. variables\n",
    "def sortobject(dataframe):\n",
    "    # Sort dataframe on increasing incidence values\n",
    "    unsorted_incidences = dataframe.incidence.values\n",
    "    rank = unsorted_incidences.argsort()\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_decimals = 2\n",
    "average = round(df_out.TARGET[df_out.PARTITION==\"train\"].mean(),n_decimals)\n",
    "\n",
    "headers_to_output = list(df_auc['variable'])\n",
    "for i,varname in enumerate(headers_to_output):\n",
    "    b_varname = 'B_'+varname\n",
    "    d_varname ='D_'+varname\n",
    "    #INCIDENCE CSV's\n",
    "    incidence_path = root+\"/data/univariate/incidence_\"+str(varname)+\".csv\"\n",
    "    groups_and_incidences = df_out.TARGET[df_out.PARTITION=='train'].groupby(df_out[b_varname]).mean()\n",
    "    n_groups= len(groups_and_incidences)\n",
    "    group = groups_and_incidences.index\n",
    "    incidence = groups_and_incidences.values.round(n_decimals)\n",
    "    size = df_out.TARGET[df_out.PARTITION=='train'].groupby(df_out[b_varname]).size().astype(float).values\n",
    "    df_incidence = pd.DataFrame( {'group':group\n",
    "                                  ,'incidence':incidence\n",
    "                                  ,'size':size\n",
    "                                  ,'average':average}\n",
    "                                ,columns=['group','incidence','size','average'])\n",
    "    if varname in numeric_headers:\n",
    "        df_incidence = df_incidence.iloc[sortnumeric(df_incidence),:]\n",
    "    elif varname in object_headers:\n",
    "        df_incidence = df_incidence.iloc[sortobject(df_incidence),:]\n",
    "    else:\n",
    "        a=1\n",
    "        #df_incidence = df_incidence.iloc[sortother(df_incidence),:]\n",
    "    df_incidence.to_csv(path_or_buf=incidence_path\n",
    "                        ,sep=';'\n",
    "                        ,index=False\n",
    "                        ,encoding='utf-8'\n",
    "                        ,line_terminator='\\n') #quoting=csv.QUOTE_NONNUMERIC\n",
    "    \n",
    "    #CORRELATION CSV's\n",
    "    correlation_path = root+\"/data/univariate/correlations_\"+str(varname)+\".csv\"\n",
    "    Variable = [v.strip(\"D_\") for v in df_corr[d_varname].index]\n",
    "    Correlation = abs(df_corr[d_varname].values).round(n_decimals)\n",
    "    Sign = np.array([\"+\",\"-\"])[(df_corr[d_varname].values<0).astype(int)]\n",
    "    AUC = np.array([df_auc.loc[df_auc['variable']== v,'AUC test'].values[0] for v in Variable]).round(n_decimals)\n",
    "    df_correlation = pd.DataFrame({\"Variable\":Variable\n",
    "                                   ,\"Correlation\":Correlation\n",
    "                                   ,\"Sign\":Sign\n",
    "                                   ,\"AUC\": AUC}\n",
    "                                  ,columns=[\"Variable\",\"Correlation\",\"Sign\",\"AUC\"]) \n",
    "    df_correlation.sort_values(by='Correlation', ascending=False, inplace=True)\n",
    "    df_correlation = df_correlation.loc[df_correlation.Variable!=varname,:]\n",
    "    df_correlation.to_csv(path_or_buf=correlation_path\n",
    "                          ,sep=';'\n",
    "                          ,index=False\n",
    "                          ,encoding='utf-8'\n",
    "                          ,line_terminator='\\n') # quoting=csv.QUOTE_NONNUMERIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Preselections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selections_path = root+'/data/univariate/variable_selections.csv'\n",
    "df_variable_selections.to_csv(path_or_buf=selections_path\n",
    "                              ,sep=';'\n",
    "                              ,index=False\n",
    "                              ,encoding='utf-8'\n",
    "                              ,line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result dataframe for Modeling input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = root+\"/data/univariate/df_univariate.csv\"\n",
    "df_out.to_csv(path_or_buf=out_path, sep=';', index=False, encoding='utf-8', line_terminator='\\n', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeltab reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate modeltab info\n",
    "filename = root+\"/data/univariate/modeltab_info.csv\"\n",
    "with open(filename, 'w') as csvfile:\n",
    "    write=csv.writer(csvfile, delimiter =';')\n",
    "    write.writerow([\"key\",\"value\"])\n",
    "    write.writerow([\"run\",\"Default\"])\n",
    "    write.writerow([\"new\",\"Alternative 1\"])\n",
    "    write.writerow([\"new_template\",\"Default\"])\n",
    "    write.writerow([\"champ\",\"Default\"])\n",
    "    write.writerow([\"score\",\"Default\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.append(\"-- Univariate analysis completed --\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(root+'/python/univariate.log','w')\n",
    "log_file.write('\\n'.join(log))\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
