{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from itertools import chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When code is in script, we define the path of the script's parent folder location as the root directory\n",
    "# From this root we can travel to the relevant folders with minimal adjustment\n",
    "try:\n",
    "    root = os.path.dirname(os.path.realpath(__file__))\n",
    "    root = \"/\".join(root.split('\\\\')[:-1])\n",
    "    log.append('Dynamic paths'+'\\n')\n",
    "except:\n",
    "    root = 'C:/wamp64/www/python_predictions_4/assets/scripts'\n",
    "    log.append('Static paths'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To allow pandas dataframes to display more columns\n",
    "pd.set_option(\"display.max_columns\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and organize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read-in univariate output with asssumed ID, TARGET, PARTITION and D_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univariate_path = root+\"/data/univariate/df_univariate.csv\"\n",
    "df_in = pd.read_csv(df_univariate_path, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference X and Y for each partition individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvars = [n for n in df_in.columns if n[:2] == 'D_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_train = df_in.PARTITION==\"train\"\n",
    "mask_selection = df_in.PARTITION==\"selection\"\n",
    "mask_validation = df_in.PARTITION==\"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_in.loc[mask_train,'TARGET']\n",
    "y_selection = df_in.loc[mask_selection,'TARGET']\n",
    "y_validation = df_in.loc[mask_validation,'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df_in.loc[mask_train,dvars]\n",
    "x_selection = df_in.loc[mask_selection,dvars]\n",
    "x_validation = df_in.loc[mask_validation,dvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_settings = pd.read_csv(root+'/python/analysis_settings.csv', sep=',', index_col=0, header=None).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeltab info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_modeltab = pd.read_csv(root+'/data/univariate/modeltab_info.csv',sep=';', index_col=0, header=None).T\n",
    "modelrun = df_modeltab.run[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_selections = pd.read_csv(root+'/data/univariate/variable_selections.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model making and recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make logistic model on a predefined set of predictors + compute train AUC of resulting model \n",
    "def processSubset(predictors_subset):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    # Fit model on predictors_subset and retrieve performance metric\n",
    "    model = LogisticRegression(fit_intercept=True, C=1e9, solver = 'liblinear')\n",
    "    modelfit = model.fit(y=y_train, X=x_train[predictors_subset])\n",
    "    # Position of the TARGET==1 class\n",
    "    pos = [i for i,h in enumerate(modelfit.classes_) if h==1]\n",
    "    # Prediction probabilities for the TARGET==1\n",
    "    y_pred = modelfit.predict_proba(x_train[predictors_subset])[:,pos]\n",
    "    auc = metrics.roc_auc_score(y_true=y_train, y_score=y_pred)\n",
    "    return {\"modelfit\":modelfit,\"auc\":auc,\"predictor_names\":predictors_subset,\"predictor_lastadd\":predictors_subset[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for computing AUC of all sets (train, selection & validation)\n",
    "def getAuc(df_without_auc):\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "    df_with_auc = df_without_auc[:]\n",
    "    for x,y,part in [(x_train,y_train,'train'),\n",
    "                    (x_selection,y_selection,'selection'),\n",
    "                    (x_validation,y_validation,'validation')]:\n",
    "        pos = [i for i,h in enumerate(df_without_auc.modelfit.classes_) if h==1]\n",
    "        y_pred = df_without_auc.modelfit.predict_proba(x[df_without_auc['predictor_names']])[:,pos]\n",
    "        df_with_auc[\"auc_\"+part] = metrics.roc_auc_score(y_true=y, y_score=y_pred)\n",
    "        df_with_auc[\"pred_\"+part] = y_pred\n",
    "    return(df_with_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward selection function that uses processSubset and getAuc\n",
    "def forward(current_predictors, pool_predictors, positive_only=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    tic = time.time()\n",
    "    \n",
    "    #Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in pool_predictors if p not in current_predictors]\n",
    "    # If there are no more predictors left to use, raise an error we can easily identify as normal\n",
    "    if len(remaining_predictors)==0:\n",
    "        raise ValueError(\"No more predictors left to use\",\"NormalStop\")\n",
    "    \n",
    "    #Create a model for each combination of: current predictor(s) + one of the remaining predictors\n",
    "    #Keep track of the submodels and their performance\n",
    "    #If error skip to next and do not include in comparison table\n",
    "    results = []\n",
    "    errorcount = 0\n",
    "    for p in remaining_predictors:\n",
    "        try:\n",
    "            results.append(processSubset(current_predictors+[p]))\n",
    "        except:\n",
    "            errorcount += 1 \n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # If we require all coefficients to be positive...\n",
    "    if positive_only:\n",
    "        #Create a flag for each submodel to test if all coefficients are positive \n",
    "        all_positive = pd.Series(None, index=models.index)\n",
    "        for i in range(0,len(models)):\n",
    "            all_positive[i] = (models.modelfit[i].coef_ >= 0 ).all()\n",
    "            \n",
    "        # if no model exist with only positive coefficients raise error we can easily identify as normal\n",
    "        if (all_positive==0).all():\n",
    "            raise ValueError(\"No models with only positive coefficients\",\"NormalStop\")\n",
    "            \n",
    "        #Choose model with best performance and only positive coefficients\n",
    "        best_model = models.loc[models[all_positive==1].auc.argmax()]\n",
    "        best_model = getAuc(best_model)\n",
    "        \n",
    "    # If we don't require all coefficients to be positive...   \n",
    "    else:\n",
    "        #Choose model with best performance\n",
    "        best_model = models.loc[models.auc.argmax()]\n",
    "        best_model = getAuc(best_model)\n",
    "\n",
    "    \n",
    "    tac = time.time()\n",
    "    info = (\"Processed \"\n",
    "            + str(models.shape[0])\n",
    "            + \" models on \"\n",
    "            + str(len(current_predictors)+1) \n",
    "            + \" predictors in \" \n",
    "            + str(round(tac-tic,2)) \n",
    "            +\" sec with \" \n",
    "            + str(errorcount) \n",
    "            +\" errors\")\n",
    "    \n",
    "    return best_model, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create recipient vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.DataFrame(columns=[\"modelfit\",\n",
    "                                    \"predictor_names\",\n",
    "                                    \"predictor_lastadd\",\n",
    "                                    \"auc_train\",\n",
    "                                    \"auc_selection\",\n",
    "                                    \"auc_validation\",\n",
    "                                    \"pred_train\",\n",
    "                                    \"pred_selection\",\n",
    "                                    \"pred_validation\"])\n",
    "predictors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of steps depending on settings and total number of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_setting = int(df_settings.modeling_nsteps)\n",
    "n_steps = min(step_setting,len(x_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which variables to pass, force and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pass = (df_selections.preselect == 1) & (df_selections[modelrun]==0)\n",
    "varname_list_pass = 'D_'+df_selections.loc[mask_pass,'variable']\n",
    "length_pass = len(varname_list_pass)\n",
    "mask_force = (df_selections.preselect == 1) & (df_selections[modelrun]==1)\n",
    "varname_list_force = 'D_'+df_selections.loc[mask_force,'variable']\n",
    "length_force = len(varname_list_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute forward modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "use_predictors = varname_list_force #x_train.columns\n",
    "for i in range(1,n_steps+1):\n",
    "    try:\n",
    "        # Use predictors to be forced first. Once through the list, append the remaining variables to be passed.\n",
    "        use_predictors = varname_list_force.append(varname_list_pass[[i>length_force]*length_pass]).reset_index(drop=True)\n",
    "        result = forward(current_predictors=predictors\n",
    "                         ,pool_predictors= use_predictors\n",
    "                         ,positive_only=True)\n",
    "        best_models.loc[i] = result[0]\n",
    "        predictors = best_models.loc[i].predictor_names\n",
    "        log.append(result[1])\n",
    "    except Exception as e:\n",
    "        # Normal errors (i.e. no more predictors to be used / no models with only positive coefficients)\n",
    "        if e.args[-1]=='NormalStop':\n",
    "            log.append(\"Stopped modeling at \"+str(i)+\" predictors: \"+ e.args[-2])\n",
    "        # Other unknown errors\n",
    "        else:\n",
    "            log.append(\"Stopped modeling at \"+str(i)+\" predictors: unknown error\")\n",
    "        break\n",
    "toc = time.time()\n",
    "log.append(\"Forward selection modeling: \" + str(round((toc-tic)/60,0)) + \" min\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = range(1,n_steps+1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_predictors = varname_list_force.append(varname_list_pass[[i>length_force]*length_pass]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_predictors=predictors\n",
    "pool_predictors= use_predictors\n",
    "positive_only=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_predictors = [p for p in pool_predictors if p not in current_predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "errorcount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in remaining_predictors:\n",
    "    try:\n",
    "        results.append(processSubset(current_predictors+[p]))\n",
    "    except:\n",
    "        errorcount += 1 \n",
    "models = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_positive = pd.Series(None, index=models.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(models)):\n",
    "    all_positive[i] = (models.modelfit[i].coef_ >= 0 ).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (all_positive==0).all():\n",
    "    raise ValueError(\"No models with only positive coefficients\",\"NormalStop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = models.loc[models[all_positive==1].auc.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_without_auc = best_model\n",
    "df_with_auc = df_without_auc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [i for i,h in enumerate(df_without_auc.modelfit.classes_) if h==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_validation\n",
    "y = y_validation\n",
    "part = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean() # THIS IS WHY THERE IS AN ERROR. ONLY ZEROS IN THE VALIDATION SET.............. ????!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_without_auc.modelfit.predict_proba(x[df_without_auc['predictor_names']])[:,pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_true=y, y_score=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal model criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparefit(p,g=2):\n",
    "    # We fit a second degree (g=2) polyline through our auccurve \n",
    "    # This serves as a starting base for finding our optimal stopping point\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    z = np.polyfit(p.index, p, g)\n",
    "    f = np.poly1d(z)\n",
    "    y_new = f(p.index)\n",
    "    return pd.Series(y_new,index=p.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slopepoint(p,p_fit,thresh_ratio=0.2):\n",
    "    # We take the polyline from comparefit and look for the point of which the slope lies just below some percentage of the max. slope\n",
    "    slopes = [p_fit[i+1]-p_fit[i] for i in range(1,len(p_fit))]\n",
    "    slopes = pd.Series(slopes, index=range(1,len(p_fit)))\n",
    "    thresh = slopes.max()*thresh_ratio\n",
    "    p_best_index = (slopes[slopes>thresh])[-1:].index\n",
    "    p_best = p.loc[p_best_index]\n",
    "    return p_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moveright(p,p_fit,p_best,n_steps=5,dampening=0.01):\n",
    "    # We look nsteps right on the polyline (starting from the slopepoint) and take the point with largest difference with real line\n",
    "    # We move to that point if that difference is larger than some multiplication of the difference at the slopepoint\n",
    "    # That multiplication gets larger as current the current difference gets smaller with a certain amount of dampening. \n",
    "    # The rationale behind this is as follows: \n",
    "    #  if the current difference is already large than the larger difference will definitely be noteworthy\n",
    "    #  if however the current difference is near zero than there needs to be much larger difference to be noteworthy\n",
    "    in_index = p_best.index.values[0]\n",
    "    lower = (in_index-1)\n",
    "    upper = (in_index+n_steps-1)\n",
    "    p_diff = p[lower:upper]-p_fit[lower:upper]\n",
    "    out_index = p_diff.argmax()\n",
    "    factor = 1/abs(p_diff[in_index])\n",
    "    if (p_diff[out_index]>p_diff[in_index]+(abs(p_diff[in_index])*factor*dampening)):\n",
    "        p_best_new = pd.Series(p[out_index],index=[out_index])\n",
    "    else:\n",
    "        p_best_new = p_best\n",
    "    return p_best_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moveleft(p,p_fit,p_best,rangeloss=0.1, diffshare=0.8): #diff_min=0.005):\n",
    "    # Starting from whatever point we end up with (either the slopepoint or a move to the right)\n",
    "    # We look left on the polyline and take the point for which the real line is largest (current point included)\n",
    "    # We move left if we stay within [a specific % loss of range] AND [a minimum % of current difference]\n",
    "    # i.e. we don't won't to go to low compared to the overall real line\n",
    "    #      and we don't won't to move to a point that does not make a significant increase in AUC (i.e. difference between polyline and real line)\n",
    "    p_left = p[:p_best.index.values[0]]\n",
    "    p_best = p_left[p_left==p_left.max()]\n",
    "    p_diff = p-p_fit\n",
    "    p_range = p.max()-p.min()\n",
    "    s = p[(p >= p_best.values[0]-(rangeloss*p_range)) \n",
    "          & (p.index <= p_best.index.values[0]) \n",
    "          & (p_diff>=diffshare*p_diff[p_left.index[-1]])\n",
    "         ]\n",
    "    p_best_new = s[s.index == s.index.values.min()]\n",
    "    return p_best_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = best_models.auc_selection\n",
    "points_fit = comparefit(p=points, g=2)\n",
    "points_slope = slopepoint(p=points, p_fit=points_fit, thresh_ratio=0.2)\n",
    "points_right = moveright(p=points, p_fit=points_fit, p_best=points_slope, n_steps=5, dampening=0.01)\n",
    "points_left = moveleft(p=points, p_fit=points_fit, p_best=points_right, rangeloss=0.1, diffshare=0.8)\n",
    "\n",
    "optimal_nvars = points_left.index.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "plt.plot( points.index      , points          , color=\"blue\")\n",
    "plt.plot( points_fit.index  , points_fit      , color=\"red\")\n",
    "plt.plot( points_slope.index, points_slope,'o', color=\"lightgreen\", markersize=12)\n",
    "plt.plot( points_right.index, points_right,'o', color=\"black\"     , markersize=8)\n",
    "plt.plot( points_left.index , points_left ,'o', color=\"gold\"      ,  markersize=4)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.45,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative gains/response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative response/gains\n",
    "def cumulatives(y,yhat,perc_as_int=False,dec=2):\n",
    "    nrows = len(y)\n",
    "    npositives = y.sum()\n",
    "    y_yhat = pd.DataFrame({\"y\":y, \"yhat\":yhat}).sort_values(by='yhat', ascending=False).reset_index(drop=True)\n",
    "    cresp = []\n",
    "    cgains = [0]\n",
    "    for stop in (np.linspace(0.01,1,100)*nrows).astype(int):\n",
    "        cresp.append(round(y_yhat.loc[:stop,'y'].mean()*max(100*int(perc_as_int),1),dec))\n",
    "        cgains.append(round(y_yhat.loc[:stop,'y'].sum()/npositives*max(100*int(perc_as_int),1),dec))\n",
    "    return cresp,cgains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cresp_all = [None]\n",
    "cgains_all = [None]\n",
    "for i in range(1,len(best_models)+1):\n",
    "    out = cumulatives(y=y_selection\n",
    "                      ,yhat=best_models.pred_selection[i][:,0]\n",
    "                      ,perc_as_int=True\n",
    "                      ,dec=2)\n",
    "    cresp_all.append(out[0]) \n",
    "    cgains_all.append(out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "cmap = plt.get_cmap('hot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n_steps)]\n",
    "for i in range(1,len(best_models)):\n",
    "    plt.plot(range(1,101), cresp_all[i], color=colors[i-1])\n",
    "plt.plot(range(1,101), cresp_all[-1], color=\"black\")\n",
    "            \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,max(max(l) for l in np.array(cresp_all)[1:])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "cmap = plt.get_cmap('hot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n_steps)]\n",
    "for i in range(1,len(best_models)):\n",
    "    plt.plot(range(0,101), cgains_all[i], color=colors[i-1])\n",
    "plt.plot(range(0,101), cgains_all[-1], color=\"black\")\n",
    "            \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,max(max(l) for l in np.array(cgains_all)[1:])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute variable importance based on correlation between predictor and prediction (on selection set)\n",
    "def getImportance(model):\n",
    "    from scipy import stats\n",
    "    \n",
    "    predictors = [pred[2:] for pred in model.predictor_names]\n",
    "    pearcorr = []\n",
    "    for predictor in predictors:\n",
    "        pearsonr = stats.pearsonr(x_selection.loc[:,'D_'+predictor].values, model.pred_selection[:,0])\n",
    "        pearcorr.append(pearsonr[0].round(2))\n",
    "    df_result = pd.DataFrame({'variable':predictors,'importance':pearcorr}, columns=['variable','importance'])\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_all=[None]\n",
    "for i in best_models.index:\n",
    "    importance_all.append(getImportance(best_models.loc[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "#nvars = optimal_nvars\n",
    "nvars =  len(best_models)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "predictors = importance_all[nvars].variable\n",
    "y_pos = np.arange(len(predictors))\n",
    "importance = importance_all[nvars].importance\n",
    "\n",
    "ax.barh(y_pos, importance, align='center',\n",
    "        color='darkblue', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(predictors)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store all variable names + coefficients for every best model (with 1,2,3,... variables) \n",
    "vars_out = []\n",
    "coef_out = []\n",
    "nmod_out = []\n",
    "for i in best_models.index:\n",
    "    modout = best_models.loc[i,:]\n",
    "    vars_out_st = ['Intercept']+[var[2:] for var in modout.predictor_names]\n",
    "    vars_out.append(vars_out_st)\n",
    "    coef_out_st = list(modout.modelfit.intercept_)+list(+ modout.modelfit.coef_[0])\n",
    "    coef_out.append(coef_out_st)\n",
    "    nmod_out.append([i]*(i+1))\n",
    "    \n",
    "vars_out = list(chain.from_iterable(vars_out))\n",
    "coef_out = list(chain.from_iterable(coef_out))\n",
    "nmod_out = list(chain.from_iterable(nmod_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coeff = pd.DataFrame({'nstep':nmod_out,'varname':vars_out,'coeff':coef_out}, columns=['nstep','varname','coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmods = len(best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = root+\"/data/modeling/\"+modelrun+\"_auccurve.csv\"\n",
    "with open(filename, 'w') as csvfile:\n",
    "    write=csv.writer(csvfile, delimiter =';')\n",
    "    write.writerow([\"optimal\" ,optimal_nvars])\n",
    "    write.writerow([\"selected\",optimal_nvars])\n",
    "    write.writerow([\"variable\",\"train\", \"selection\",\"validation\"])\n",
    "    write.writerows([best_models.predictor_lastadd[i][2:]\n",
    "                     , best_models.auc_train[i].round(3) \n",
    "                     , best_models.auc_selection[i].round(3)\n",
    "                     , best_models.auc_validation[i].round(3) ] for i in range(1,nmods+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cresp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in range(1,nmods+1):\n",
    "    filename = root+\"/data/modeling/\"+modelrun+\"_cresp_\"+str(v)+\".csv\"\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        write=csv.writer(csvfile, delimiter =';') \n",
    "        write.writerows([i+1, cresp_all[v][i]] for i in range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cgains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in range(1,nmods+1):\n",
    "    filename = root+\"/data/modeling/\"+modelrun+\"_cgains_\"+str(v)+\".csv\"\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        write=csv.writer(csvfile, delimiter =';') \n",
    "        write.writerows([i, cgains_all[v][i]] for i in range(0,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(1,nmods+1):\n",
    "    filename = root+\"/data/modeling/\"+modelrun+\"_importance_\"+str(v)+\".csv\"\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        write=csv.writer(csvfile, delimiter =';') \n",
    "        write.writerow(['variable','importance'])\n",
    "        write.writerows([importance_all[v].iloc[i,0],importance_all[v].iloc[i,1]] for i in range(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = root+\"/data/modeling/\"+modelrun+\"_modelcoeff.csv\"\n",
    "df_coeff.to_csv(path_or_buf=out_path, sep=';', index=False, encoding='utf-8', line_terminator='\\n', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log.append(\"-- Modeling phase completed --\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(root+\"/python/\"+modelrun+\"_modeling.log\",'w')\n",
    "log_file.write('\\n'.join(log))\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring of all rows\n",
    "import re\n",
    "tic = time.time()\n",
    "df_score = pd.DataFrame([])\n",
    "df_score['ID'] = df_in['ID']\n",
    "scores = []\n",
    "for i in range(len(df_in)):\n",
    "    ### METHOD 1: using function\n",
    "    score = [optifit.predict_proba(df_in[optivars])[i,:][-1]]\n",
    "    ### METHOD 2: with coefficients (same method as in scoring)\n",
    "    #exponent = optiint + ((df_in[optivars].iloc[i,:])*(opticoef[0])).sum()\n",
    "    #score = [(math.exp(exponent)) / (1+math.exp(exponent))]\n",
    "    \n",
    "    scores.extend(score)\n",
    "    try:\n",
    "        zeros = re.findall('[0]+$',str(i))\n",
    "        if len(zeros[0])>=3:\n",
    "            print(i)\n",
    "    except:\n",
    "        a=1\n",
    "df_score['score']=pd.Series(scores)\n",
    "tac = time.time()\n",
    "print((tac-tic)/60)\n",
    "\n",
    "\n",
    "df_in.to_csv('df_mod.csv', sep=';', index=False, encoding='utf-8', line_terminator='\\n')\n",
    "df_score.to_csv(path_or_buf='scores_modeling.csv', sep=';', index=False, encoding='utf-8', line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /WIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
